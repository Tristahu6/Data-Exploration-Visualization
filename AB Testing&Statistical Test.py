#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np 


# # A/B Testing Sales Promotion Strategies
df_campaign = pd.read_csv('campaign_data.csv')
df_campaign

# A 
# Generate a barplot to show the average SalesInThousands values, separated by the different promotion types
sns.barplot(x='Promotion', y='SalesInThousands', data=df_campaign, ci=None)

plt.xlabel('Promotion Type')
plt.ylabel('Average Sales (in Thousands)')
plt.title('Average Sales by Promotion Type')

plt.show()
# The barplot visualizes the average SalesInThousands values generated by each promotion type. 
# It shows that the average sales are similar for Promotion 1 (around 58,000), Promotion 2 (around 61,000), and Promotion 3 (around 55,000).

df_campaign['week'] = df_campaign['week'].astype('category')

sns.countplot(x='Promotion', hue='week', data=df_campaign)

plt.xlabel('Promotion Type')
plt.ylabel('Number of Instances')
plt.title('Number of Instances per Promotion Type')

plt.show()
# As the gragh shows, the number of instances for each campaign is the same for every week, 
# this suggests that the experiment design has an even distribution of promotions across time. This indicates a well-controlled experimental design where the 'week' variable is not confounded with the 'PromotionType' variable. In other words, the 'week' variable is not a confounding variable and it does not influence or interact with the promotion types. This design ensures that any observed differences in sales or outcomes can be more confidently attributed to the variation in the promotion types rather than the effect from 'week'.

summary_stats = df_campaign.groupby('Promotion')['AgeOfStore'].describe()

print(summary_stats)

# There are 172 stores in Promotion 1, 188 stores in Promotion 2, and 188 stores in Promotion 3. The count of stores is similar across the three groups, suggesting that the sample sizes are relatively balanced.
# 
# Overall, based on these results, it appears that the age profile of the stores does not seem to be very different across the three groups. The average sales, variability (similar Standard Deviation), and range of sales are similar, reducing the risk of age profile becoming a confounding variable to the experiment. 

import scipy.stats as stats

# Extract the sales values for each promotion 
sales_promo1 = df_campaign[df_campaign['Promotion'] == 1]['SalesInThousands']
sales_promo2 = df_campaign[df_campaign['Promotion'] == 2]['SalesInThousands']
sales_promo3 = df_campaign[df_campaign['Promotion'] == 3]['SalesInThousands']

# Perform pairwise t-tests
t_statistic_1_vs_2, p_value_1_vs_2 = stats.ttest_ind(sales_promo1, sales_promo2)
t_statistic_2_vs_3, p_value_2_vs_3 = stats.ttest_ind(sales_promo2, sales_promo3)
t_statistic_1_vs_3, p_value_1_vs_3 = stats.ttest_ind(sales_promo1, sales_promo3)

print("Promotion 1 vs. Promotion 2:")
print("T-Statistic:", t_statistic_1_vs_2)
print("P-Value:", p_value_1_vs_2)

print("\nPromotion 2 vs. Promotion 3:")
print("T-Statistic:", t_statistic_2_vs_3)
print("P-Value:", p_value_2_vs_3)

print("\nPromotion 1 vs. Promotion 3:")
print("T-Statistic:", t_statistic_1_vs_3)
print("P-Value:", p_value_1_vs_3)

 
# The t-statistics and p-statistics for each pair is as below: 
# 
# Promotion 1 vs. Promotion 2:
# T-Statistic: -2.5230805833394934
# P-Value: 0.012065595012565785
# 
# Promotion 2 vs. Promotion 3:
# T-Statistic: 4.2191398016523785
# P-Value: 3.0807459102757044e-05
# 
# Promotion 1 vs. Promotion 3:
# T-Statistic: 1.5551383687293547
# P-Value: 0.12079667272313273

# D-b.
# 
# Promotion 1 vs. Promotion 2: The negative t-statistic of -2.523 suggests that Promotion 1 has, on average, lower sales than Promotion 2. In other words, Promotion 2 performs better than Promotion 1 in terms of sales. The p-value (0.0121) is less than the default significance level of 0.05. This indicates that there is strong evidence to conclude a statistically significant difference between the means of Promotion 1 and Promotion 2.
# 
# Promotion 2 vs. Promotion 3: The positive t-statistic of 4.219 suggests that the mean sales for Promotion 2 are higher than the mean sales for Promotion 3. This relatively large t-statistic indicates a big difference between the two promotions. The extremely small p-value (3.0807459102757044e-05) provides strong evidence to reject the null hypothesis and conclude that Promotion 2 significantly outperforms Promotion 3 in terms of sales.
# 
# Promotion 1 vs. Promotion 3: The t-statistic of 1.555 suggests a modest difference in mean sales between Promotion 1 and Promotion 3. However, the p-value of 0.12079667272313273 exceeds the significance level of 0.05, indicating that the observed difference is not statistically significant. Therefore, there is insufficient evidence to conclude that there is a meaningful difference between Promotion 1 and Promotion 3 in terms of sales.

# # Part III: Using a Statistical Test to Evaluate a Claim

import numpy as np
from scipy.stats import chisquare

# A
# Recorded values
dice_values = np.array([1, 2, 3, 4, 5, 6])
observed_values = np.array([13, 7, 12, 8, 14, 6])

# Expected values 
expected_values = np.ones_like(dice_values) * (60 / 6)

# Perform chi-square goodness of fit test
chi2_stat, p_value = chisquare(observed_values, expected_values)

print("Chi-square statistic:", chi2_stat)
print("P-value:", p_value)


# A-a
# 
# The null hypothesis (H0) of the test in this case is that the observed dice rolls are fair thus follow the expected frequencies for a fair six-sided dice. 
# 
# The alternative hypothesis (Ha) is that the observed frequencies deviate significantly from the expected frequencies to be observed from a fair dice.

# A-b
# 
# Assuming that Lobster Land uses an alpha value of 0.05 for statistical tests, with a chi-square statistic of 5.800 and a p-value of 0.326, which is bigger than 0.05, we fail to reject the null hypothesis (H0: the observed dice rolls are fair thus follow the expected frequencies for a fair six-sided dice) in this case. Therefore, based on this test, we can not conclude that the observed frequencies of the dice rolls significantly deviate from the expected frequencies for a fair six-sided dice. Thus we cannot conclude whether the dice used are biased or unfair.

# In[19]:


# B
# Recorded values
observed_values_120 = np.array([26, 14, 24, 16, 28, 12])

expected_values_120 = np.ones_like(dice_values) * (120 / 6)

# Perform chi-square goodness of fit test
chi2_stat_120, p_value_120 = chisquare(observed_values_120, expected_values_120)

print("Chi-square statistic:", chi2_stat_120)
print("P-value:", p_value_120)


# B-a.
# 
# The null hypothesis (H0) of the test in this case is that the observed dice rolls are fair thus follow the expected frequencies for a fair six-sided dice. 
# 
# The alternative hypothesis (Ha) is that the observed frequencies deviate significantly from the expected frequencies to be observed from a fair dice.

# B-b.
# 
# Assuming that Lobster Land uses an alpha value of 0.05 for statistical tests, with a chi-square statistic of 11.600 and a p-value of 0.041, which is less than the chosen significance level (0.05), we have evidence to reject the null hypothesis (H0: that the observed dice rolls are fair thus follow the expected frequencies for a fair six-sided dice). This indicates that the observed frequencies deviate significantly from what would be expected from a fair six-sided die. Therefore, we can conclude that the dice is not fair.

# C-i.
# 
# Firstly, in both trials, the numbers 1, 3, and 5 appeared more frequently than 2, 4, and 6. Per the rule, if the roll results in 1, 3, or 5, the visitor will lose $12. 
# 
# Secondly, trial A involved 60 dice rolls and did not provide evidence of bias or unfairness in the dice. However, trial B, which included 120 dice rolls, suggests that the dice used in that trial is biased. This case of false negative in trial A, indicating that we should remain open to alternative hypotheses and be aware that the sample size can influence the test outcome and may fail to reject the null hypothesis.

# C-ii
# 
# To generate a chi-square statistic, we compare observed and expected values for each category.  We square the differences between the observed and expected values, and then divide that result by the expected value. The reason of the result difference could be sample size. A larger sample size provides more data points, thus can lead to more precise estimates of the expected frequencies and observed frequencies. With a smaller sample size, even small deviations from the expected frequencies can become statistically significant, making it less likely to miss detecting a true effect or bias. 
# 
# Additional, When the number of dice rolls is small, the observed frequencies may be more influenced by random variation. With smaller sample sizes, Random fluctuations can have a larger impact on the observed frequencies. Consequently, the test may not be able to distinguish between random variation and systematic deviations.
